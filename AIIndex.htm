<!DOCTYPE html>
<html>
<head>
<title>Artificial Intelligence</title>


<!--
<style>
body {
  background-color: black;
}
h1 {text-align: center;
    color: white;}
h2 {text-align: center;
    color: white;}
h4 {text-align: center;
    color: white;}
p {text-align: left;
   color: white;}
</style>
-->

<style>
h1 {text-align: center; }
h2 {text-align: center; }
h4 {text-align: center; }
p {text-align: left; }
</style>


</head>
<body>

<!--
Instructions:
https://pages.github.com/
-->

<h1>Artificial Intelligence</h1>

<h4>Eric Chauvin: chauvine959@gmail.com
</h4>

<h4><a href="https://github.com/EricsSourceCode/">
The Source Code on GitHub</a>
</h4>

<p>A good book is Why Machines Learn,
 by Anil Ananthaswamy.  It has a good
 introduction to the math for AI and it has
 interesting history for understanding the
 background and context of ideas.  Another
 good book is Deep Learning (The MIT Press),
 by John D. Kelleher.  It has a good
 explanation of the Backpropagation Algorithm.
</p>

<p>This is
 <a href="http://neuralnetworksanddeeplearning.com/chap2.html">
 a good tutorial</a> with a good explanation
 of the Backpropagation Algorithm.
</p>

<p>This project uses news stories as the
 source of data for finding patterns using
 a Neural Network.  The main
 <a href="https://github.com/EricsSourceCode/AINews">
 project source code is here</a>.
 <a href="https://github.com/EricsSourceCode/CsNeural">
 Neural Network source code is here</a>.
</p>

<p>There is that old saying: Make it work
 first, then make it work fast.  The first
 version of the Neural Network source code has
 the number 1, like NeuralNet1.cs, then the
 next version is called NeuralNet2.cs and the
 numbers increase like that with each version.
 So each version is more optimized and more
 complex than earlier versions.  Explanations
 for how the Backpropagation
 Algorithm works are in comments in the code
 in NeuralNet1.cs and other places.
</p>

<p>The first version of source code should
 be expository.  A programming language
 is an expressive
 language that is used to explain concepts.
 Later versions can be more optimized, but
 they'd still be tested against the earlier
 versions of code.
</p>

<p>When it comes to the learning phase,
 just using the training data only, the
 more complex
 the input patterns are, the bigger and
 deeper the network will have to be.
 (Computational Complexity Theory and
 Information Theory.)  The more complex
 something is, the more random it looks,
 and if it was completely random, or
 very complex like in Cryptography, then
 during the training phase, the weights
 can't trend toward anything.  If you are old
 enough to remember seeing a TV that is not
 tuned to any channel, it is just showing
 static random noise, that is what very complex
 information looks like and so there aren't
 obvious hills and valleys for the
 Gradient Descent Algorithm to use in
 finding a minimum of the Cost Function.
 What is very interesting is that very
 deep networks with a very large number
 of parameters can find patterns that you
 might not think are there.  And they
 might not find the patterns you thought
 you wanted to look for.
</p>


<!--
What pages under the AI index?
<h4><a href="https://EricsSourceCode.github.io/TlsIndex.htm">
Transport Layer Security</a>
</h4>

-->

<p><a href="https://EricsSourceCode.github.io/">
Main Page</a>
<br><br>

</body>
</html>
